{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9x3bUdFD_zB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.24.0\n",
        "!pip install simpletransformers==0.63.11\n",
        "!pip install scikit-learn\n",
        "!pip install tensorflow\n",
        "!pip install dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_2bOS4CETez"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "from simpletransformers.classification import ClassificationModel, MultiLabelClassificationModel\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeOcmgLoETgl"
      },
      "outputs": [],
      "source": [
        "import sys, os, re, json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Tuple, Dict\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGIGsPCuHv5p"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVzIXJE1ETlq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "#\n",
        "dir_dataset = \"\"\n",
        "file_business = os.path.join(dir_dataset, \"yelp_academic_dataset_business.json\")\n",
        "file_review = os.path.join(dir_dataset, \"yelp_academic_dataset_review.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcoG_4TuETrG"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Yelp_Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXTCqivb6iNe"
      },
      "outputs": [],
      "source": [
        "''' Read in the data - copied from my code in using machine learning algorithms'''\n",
        "def json_df (file_name):\n",
        "  counter = 0\n",
        "  recorder = []\n",
        "\n",
        "  for chunk in pd.read_json(\"yelp_academic_dataset_review.json\", lines=True, chunksize=1000):\n",
        "      recorder.append(chunk)\n",
        "\n",
        "      ''' #For smaller sized chunks to test code\n",
        "      if counter == 5:\n",
        "          break\n",
        "      '''\n",
        "      counter += 1\n",
        "      if counter % 1000 == 0:\n",
        "          print(counter)\n",
        "\n",
        "  df_review = pd.concat(recorder)\n",
        "  print(\"df_review made\")\n",
        "  return df_review"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "def undersample(df, group_size=200000):\n",
        "  dfs = []\n",
        "\n",
        "  for label in df[\"stars\"].value_counts().keys():\n",
        "    df_group = df[df[\"stars\"] == label]\n",
        "    df_group_undersampled = resample(df_group,\n",
        "                                     replace=False,\n",
        "                                     n_samples=group_size,\n",
        "                                     random_state=0)\n",
        "    dfs.append(df_group_undersampled)\n",
        "\n",
        "  return pd.concat(dfs).sample(frac=1, random_state=0)"
      ],
      "metadata": {
        "id": "Ju2WE6c2ELEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3VdO5euETvT"
      },
      "outputs": [],
      "source": [
        "%time df_all = json_df(file_review)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_review = df_all.copy()\n",
        "df_review = undersample(df_review, 250000)"
      ],
      "metadata": {
        "id": "q70Qu6JJ2NRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_review['labels'] = df_review['stars'] - 1"
      ],
      "metadata": {
        "id": "RxTxYdO457T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_review['labels'] = df_review['stars'].apply(lambda x: 1 if x > 3 else 0)\n",
        "df_review = df_review[['text', 'labels']]"
      ],
      "metadata": {
        "id": "8O7XwNYt1rzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df_review, test_size=500000,random_state=42, shuffle=True)\n",
        "df_train, df_dev = train_test_split(df_train, test_size=500000, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "Rl5bXkxJD4TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize the texts\n",
        "def tokenize(df):\n",
        "    return tokenizer(df['text'], padding='max_length', truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "oGKQqPYADDcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "unique_labels = df_train['labels'].unique()\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(unique_labels)+1)\n",
        "\n",
        "\n",
        "# Load your dataframe into a Hugging Face dataset\n",
        "from datasets import Dataset\n",
        "\n",
        "df_new = df_train[['text', 'labels']] # training\n",
        "#df_new = df_new[:1000]\n",
        "\n",
        "dataset = Dataset.from_pandas(df_new)\n",
        "\n",
        "# Tokenize the dataset\n",
        "dataset = dataset.map(tokenize, batched=True, batch_size=len(dataset))\n",
        "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Split the dataset into a training and validation set\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy='epoch',  # or 'epoch' if you want to evaluate at the end of each epoch\n",
        "    save_steps = 10000,\n",
        "    eval_steps = 10000,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "\n",
        "# Create the Trainer and train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "oOzrTt6h4OBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Select the part of the dataframe you want to use for evaluation\n",
        "test_eval = df_review[1000:1010]\n",
        "\n",
        "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
        "test_eval = Dataset.from_pandas(test_eval)\n",
        "\n",
        "# Tokenize the data and set the format\n",
        "test_eval = test_eval.map(tokenize, batched=True, batch_size=len(test_eval))\n",
        "test_eval.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "mCjDiM5-CIJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df_test[['text', 'labels']]\n",
        "df_test_dataset = Dataset.from_pandas(df_new)\n",
        "df_test_dataset = df_test_dataset.map(tokenize, batched=True, batch_size=len(dataset))\n",
        "\n",
        "df_test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "id": "DW68ofP14OD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_output = trainer.predict(df_test_dataset)"
      ],
      "metadata": {
        "id": "yn4Eav5lgLQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = prediction_output.predictions\n",
        "\n",
        "y_pred = prediction_output.label_ids\n",
        "pred_metrics = prediction_output.metrics"
      ],
      "metadata": {
        "id": "3LVTNA0m4OGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_test['labels']\n",
        "label = 'testing'\n",
        "model_name = 'bert sentiment-only'"
      ],
      "metadata": {
        "id": "_9XXvhO4hPYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)\n",
        "print(label + ' Set')\n",
        "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
        "print()\n",
        "\n",
        "print(classification_report(y, y_pred, digits=4))\n",
        "df_cm = pd.DataFrame(confusion_matrix(y, y_pred, normalize='true'),range(1,6), range(1,6))\n",
        "#df_cm = pd.DataFrame(confusion_matrix(y, y_pred, normalize='true'),range(1,3), range(1,3))\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sn.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, square=True)\n",
        "ax.set_xlabel('Predicted label')\n",
        "ax.set_ylabel('True label')\n",
        "plt.savefig(model_name + \"_\" + label.lower() + \".eps\")\n",
        "plt.show()\n",
        "print()"
      ],
      "metadata": {
        "id": "puWsxEk24OJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}